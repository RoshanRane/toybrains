{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f1c164f-8b3f-40b1-987a-b1df74344fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BASELINE PIPELINE\n",
    "> Dev log (format < Date > | <Author(s)> )  \n",
    "> - Created: 28 July 2023 | JiHoon Kim <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dd50d9e-86b0-4be9-b5d9-985cbf3333b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4981f938-a50f-45db-915c-e5191d9236ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mcuda02                    \u001b[m  Mon Jul 31 08:56:07 2023  \u001b[1m\u001b[30m470.199.02\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 30'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m14871\u001b[m / \u001b[33m24268\u001b[m MB | \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m813M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m2133M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m3569M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m895M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m2249M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m1219M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m1057M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m1869M\u001b[m) \u001b[1m\u001b[30mtomasz\u001b[m(\u001b[33m1057M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[1m\u001b[31m 62'C\u001b[m, \u001b[1m\u001b[32m 65 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m18440\u001b[m / \u001b[33m24268\u001b[m MB | \u001b[1m\u001b[30mmanuel\u001b[m(\u001b[33m1343M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2443M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2443M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2443M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2443M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2443M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2425M\u001b[m) \u001b[1m\u001b[30mjihoon\u001b[m(\u001b[33m2445M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 28'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m24268\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce RTX 3090\u001b[m |\u001b[31m 28'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    8\u001b[m / \u001b[33m24268\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m4M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "# check GPUs available and memory\n",
    "! gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12203aa5-9e8b-48a1-9c9b-6ed16ce34610",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c4e106-2fba-4ca4-8707-e4b7c1825927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add custom imports\n",
    "from create_toybrains import ToyBrainsData\n",
    "from utils.pipeline import run_baseline_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd55fb9-16b5-45c3-a4a9-48e2dd79158e",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "343af481-e437-4928-809e-ea4fc6b5a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create toybrains dataset in loop\n",
    "def generate_toybrains_list(data_dict, img=True, debug=False):\n",
    "    ''' create a toybrains using loop of dict\n",
    "    \n",
    "    PARAMETER\n",
    "    ---------\n",
    "    data_dict : dictionary\n",
    "        data_dict\n",
    "        \n",
    "    img : boolean, default : True\n",
    "        generate toybrains dataset\n",
    "        \n",
    "    debug : boolean, default : False\n",
    "        debug mode\n",
    "    \n",
    "    OUTPUT\n",
    "    ------\n",
    "    path_list : list\n",
    "        list of csv path, element is a string\n",
    "    '''\n",
    "    path_list = []\n",
    "    \n",
    "    for data_dir, args in data_dict.items():\n",
    "        n_samples, config = args['n_samples'], args['config']\n",
    "        \n",
    "        if img:\n",
    "            if debug: print(f\"Save toybrains dataset (N={n_samples}) in 'dataset/{data_dir}'\")\n",
    "            # (TODO) compatible with CLI and notebook\n",
    "            ! python create_toybrains.py --dir $data_dir -n $n_samples -c $config\n",
    "        \n",
    "        csv_path = f\"dataset/{data_dir}/toybrains_n{n_samples}.csv\"\n",
    "        path_list.append(csv_path)\n",
    "        if debug: print(f'summary can be found in {csv_path}\\n')\n",
    "\n",
    "    return path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df20f41c-fdb1-465e-af90-824ea9480d32",
   "metadata": {},
   "source": [
    "## check your config before generate toybrains dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a13486-ce35-4060-bd85-99283826eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check config file before generate toybrains it used shapes directory\n",
    "ToyBrainsData().show_current_config()\n",
    "# ToyBrainsData(config='demo').show_current_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e4dea-939e-4040-b322-911aa69fbb1e",
   "metadata": {},
   "source": [
    "## generate toybrains dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf7a159-d18b-4e67-9f13-59eff3acfe11",
   "metadata": {},
   "source": [
    "`bsp1. run manually`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f89605-8f89-443f-85fa-6987ae6079a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python create_toybrains.py -d --dir 'toybrains30k' -n 30000 -c 'demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c64856-9e15-4755-913b-52079be36e18",
   "metadata": {},
   "source": [
    "`bsp2. generate_toybrains_list`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf81119-2334-440d-bf50-b4e70f8eec86",
   "metadata": {},
   "source": [
    "It should be extended as baseline config dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec26f5-8b79-49ef-a9f2-3bcf7b03dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'toybrains1k': dict(\n",
    "        n_samples=1000,\n",
    "        config=None,\n",
    "    ),\n",
    "    'toybrains3k': dict(\n",
    "        n_samples=3000,\n",
    "        config=None,\n",
    "    ),\n",
    "    'toybrains10k': dict(\n",
    "        n_samples=10000,\n",
    "        config=None,\n",
    "        debug=True,\n",
    "    ),\n",
    "    'toybrains30k': dict(\n",
    "        n_samples=30000,\n",
    "        config=None,\n",
    "        debug=True,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693e903-43bb-4c57-a96a-4d860b04176a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RUN with generate toybrains only for img=True\n",
    "# csv_path_list = generate_toybrains_list(data_dict, img=True, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19763b77-dca6-4113-9039-06e07d3a3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_list = generate_toybrains_list(data_dict, img=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc661ac-776c-4ff9-bbdc-0658d90bcdc2",
   "metadata": {},
   "source": [
    "## run baseline pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0853ebe-4479-481e-91c3-8ed9721daeea",
   "metadata": {},
   "source": [
    "![Diagram](https://github.com/RoshanRane/toybrains/assets/39021807/50ea3447-536f-4977-803e-7ff7668a48f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5682faa-bb78-471e-bda8-92e48a34ffe6",
   "metadata": {},
   "source": [
    "pass the input file list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1dcf0-f27b-401d-95b9-df4639459049",
   "metadata": {},
   "source": [
    "`bsp1. use data_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61f0840f-f44c-41e4-a609-6eb0f4e3c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'toybrains1k': dict(\n",
    "        n_samples=1000,\n",
    "        config=None,\n",
    "    ),\n",
    "}\n",
    "csv_path_list = generate_toybrains_list(data_dict, img=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c4ca2c-0584-4e10-8928-012da39e79e5",
   "metadata": {},
   "source": [
    "`bsp2. use raw csv absolute path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9edd4-a21c-43c8-b9df-ac1bffc50765",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_list = ['/ritter/share/projects/JiHoon/toybrains/toybrains30k/toybrains_n30000.csv']\n",
    "data_dict = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17675f-53e9-4903-ae2b-480b91d8b4a3",
   "metadata": {},
   "source": [
    "`bsp3. use raw csv relative path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6728b9-1371-46d2-b707-af5bec563868",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path_list = ['toybrains30k/toybrains_n30000.csv']\n",
    "data_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc86912-a636-44f0-bbef-5c9d910d04b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aad2ef-46fd-49c4-9a25-a8426dbc9635",
   "metadata": {},
   "source": [
    "run baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd41c10-69ca-4c2e-8592-3c824f52c24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6502c4c5-d47d-4c08-b095-d268efacb5ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "time: 2023-07-31 08:56:40.467148\n",
      "Running Baseline on a file: toybrains1k/toybrains_n1000.csv\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run supervised learning\n",
      " Training data split = 800 \n",
      " Validation data split = 100 \n",
      " Test data split = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | PyTorchMLP     | 615 K \n",
      "1 | train_acc | BinaryAccuracy | 0     \n",
      "2 | val_acc   | BinaryAccuracy | 0     \n",
      "3 | test_acc  | BinaryAccuracy | 0     \n",
      "---------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.463     Total estimated model params size (MB)\n",
      "Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "`.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4975000023841858     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4975000023841858    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.46000000834465027    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.46000000834465027   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5199999809265137     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5199999809265137    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | PyTorchMLP     | 615 K \n",
      "1 | train_acc | BinaryAccuracy | 0     \n",
      "2 | val_acc   | BinaryAccuracy | 0     \n",
      "3 | test_acc  | BinaryAccuracy | 0     \n",
      "---------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.463     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training data split = 800 \n",
      " Validation data split = 100 \n",
      " Test data split = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "`.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5962499976158142     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5962499976158142    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5699999928474426     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5699999928474426    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vol/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.550000011920929     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.550000011920929    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | PyTorchMLP     | 615 K \n",
      "1 | train_acc | BinaryAccuracy | 0     \n",
      "2 | val_acc   | BinaryAccuracy | 0     \n",
      "3 | test_acc  | BinaryAccuracy | 0     \n",
      "---------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.463     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training data split = 800 \n",
      " Validation data split = 100 \n",
      " Test data split = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "`.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5049999952316284     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5049999952316284    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.46000000834465027    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.46000000834465027   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/lblbin_shp-vent/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.49000000953674316    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.49000000953674316   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training data split = 800 \n",
      " Validation data split = 100 \n",
      " Test data split = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name      | Type           | Params\n",
      "---------------------------------------------\n",
      "0 | model     | PyTorchMLP     | 615 K \n",
      "1 | train_acc | BinaryAccuracy | 0     \n",
      "2 | val_acc   | BinaryAccuracy | 0     \n",
      "3 | test_acc  | BinaryAccuracy | 0     \n",
      "---------------------------------------------\n",
      "615 K     Trainable params\n",
      "0         Non-trainable params\n",
      "615 K     Total params\n",
      "2.463     Total estimated model params size (MB)\n",
      "Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "`.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5024999976158142     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5024999976158142    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4699999988079071     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4699999988079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Restoring states from the checkpoint path at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from checkpoint at /ritter/share/projects/JiHoon/toybrains/results/toybrains1k/20230731-0856/supervised_logs/cov_sex/MLP/version_0/checkpoints/epoch=9-step=500.ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.47999998927116394    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.47999998927116394   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running a total of 2 different settings of models\n",
      "TOTAL RUNTIME: 0:05:35\n"
     ]
    }
   ],
   "source": [
    "# (TODO) parallel needed\n",
    "for csv in csv_path_list:\n",
    "    run_baseline_pipeline(csv, data_dict, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522b19c-6629-4f3e-91ee-8b835fc4a6ce",
   "metadata": {},
   "source": [
    "visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e19b53c-546d-458e-9625-257dff45aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (TODO) visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545591e-391c-43ed-adbb-61a62b205eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "    n_list, tr_acc, vl_acc, te_acc = [], [], [], []\n",
    "\n",
    "    for n_components in n_component_list:\n",
    "        data = get_reduc_loader(dataset = dataset, method = method, n_components=n_components, seed = seed)\n",
    "        \n",
    "        # run logistic regression\n",
    "    \n",
    "        print(f\"N = {n_components}\")\n",
    "        acc, _ = run_logistic_regression(data)\n",
    "        n_list.append(n_components)\n",
    "        tr_acc.append(acc[0])\n",
    "        vl_acc.append(acc[1])\n",
    "        te_acc.append(acc[2])\n",
    "        \n",
    "    plt.plot(n_list, tr_acc)\n",
    "    plt.plot(n_list, vl_acc)\n",
    "    plt.plot(n_list, te_acc)\n",
    "    \n",
    "    plt.title(f\"Accuracy with n on {label}\")\n",
    "    plt.xlabel('N component')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28dd70-d9bf-4049-90ee-babac09a38ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv(f\"logs/my_model/version_5/metrics.csv\")\n",
    "\n",
    "aggreg_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggreg_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggreg_metrics)\n",
    "df_metrics[[\"train_loss\", \"val_loss\"]].plot(\n",
    "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"Loss\"\n",
    ")\n",
    "df_metrics[[\"train_acc\", \"val_acc\"]].plot(\n",
    "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"ACC\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
