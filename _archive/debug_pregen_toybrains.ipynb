{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce61f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python packages\n",
    "import os, sys\n",
    "from math import exp, log\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed   \n",
    "import itertools\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# import functions from Toybrains utils\n",
    "TOYBRAINS_DIR = os.path.abspath('../')\n",
    "if TOYBRAINS_DIR not in sys.path: sys.path.append(TOYBRAINS_DIR)\n",
    "from create_toybrains import ToyBrainsData\n",
    "from utils.vizutils import show_contrib_table\n",
    "from utils.genutils import gen_toybrains_dataset\n",
    "from utils.genutils_all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060b8f9",
   "metadata": {},
   "source": [
    "# Toybrain dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa81f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections to run \n",
    "GEN_DATASETS=True\n",
    "N_SAMPLES=5000\n",
    "N_SAMPLES_TEST=500\n",
    "TEST_ON_OOD=True\n",
    "GEN_IMAGES=False\n",
    "DATASET_SUFFIX=''\n",
    "OVERWRITE_EXISTING=True\n",
    "VERBOSE=0\n",
    "\n",
    "VIZ_DAG=False\n",
    "VIZ_DISTS_BEFORE=False\n",
    "VIZ_DISTS_AFTER=False\n",
    "VIZ_IMG_SAMPLES=False and GEN_IMAGES\n",
    "GEN_BASELINES=True\n",
    "VIZ_BASELINE=True and GEN_BASELINES\n",
    "BASELINE_MODELS= [('LR', {}), \n",
    "                # ('LR', {'C':0.0001}), ('LR', {'C':0.0002}), ('LR', {'C':0.0005}), ('LR', {'C':0.001}), ('LR', {'penalty': None}),\n",
    "                # ('SVM', {}),\n",
    "                 ]\n",
    "BASELINE_METRICS=['balanced_accuracy', \n",
    "                  'd2', 'logodds_r2', \n",
    "                  'logodds_mae','logodds_mse'\n",
    "                 ] #,'roc_auc', 'adjusted_mutual_info_score'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e62107",
   "metadata": {},
   "source": [
    "## Debug any pre-generated Toybrains dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3f5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to visualize a different dataset configuration than the one just generated\n",
    "basefilename = 'con1_cov-90-cat'\n",
    "TWEAK_STEPS = 36\n",
    "cov_name = basefilename.split('_')[1].replace('-','_')\n",
    "config_fnames = sorted(glob(f'configs/*{basefilename}*.py'))\n",
    "assert len(config_fnames)==TWEAK_STEPS, f\"Incorrect config files found with the name {basefilename}:\\n {config_fnames}\"\n",
    "\n",
    "TEST_CON_ASSOCIATIONS = False\n",
    "VIZ_DISTS_AFTER = False\n",
    "GEN_BASELINES = True\n",
    "VIZ_BASELINE = True\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9420208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_CON_ASSOCIATIONS:\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    for config_fname in config_fnames:\n",
    "        dataset_path = f\"dataset/toybrains_n{N_SAMPLES}_{os.path.basename(config_fname).replace('.py','')}\"\n",
    "        assert os.path.exists(dataset_path+'/train/'), f\"Dataset not found at {dataset_path}\"\n",
    "        cy = int(os.path.basename(config_fname).split('_')[-1].replace('cy','').replace('.py',''))\n",
    "        df = pd.read_csv(f'{dataset_path}/train/{os.path.basename(dataset_path)}.csv')\n",
    "        print(f\"Testing the association between {cov_name} and lbl_y for cy={cy}\")\n",
    "        \n",
    "        # read out which attribute is influenced by the confounder covariate\n",
    "        toy_temp = ToyBrainsData(config_fname)\n",
    "        display(toy_temp.show_current_config(cov_name))\n",
    "\n",
    "        X = df[cov_name]\n",
    "        # convert the categorical covariate to one-hot encoding\n",
    "        if 'cat' in cov_name:\n",
    "            X = (pd.get_dummies(X)).values\n",
    "\n",
    "        ## (1) compute the prediction accuracy from the covariate to the label\n",
    "        print(\"{}  ---> lbl_y\".format(cov_name))\n",
    "\n",
    "        y = df['lbl_y']\n",
    "        y = (y=='s1').astype(bool)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        if X_train.ndim==1:\n",
    "            X_train = X_train.reshape(-1,1)\n",
    "            X_test = X_test.reshape(-1,1)\n",
    "        # print(f\"shapes of [X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}]\")\n",
    "\n",
    "        lr = LogisticRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        print(f\"\\t Acc: {(lr.score(X_test, y_test)-0.5)*100:.0f}%\")\n",
    "\n",
    "        ## (2) compute the prediction accuracy from the covariate to the attribute\n",
    "        attr_col = list(list(toy_temp.rules[cov_name].items())[0][1].keys())\n",
    "        attr_col.remove('lbl_y')\n",
    "        attr_col = attr_col[0]\n",
    "        print(\"{}  ---> {}\".format(cov_name, attr_col))\n",
    "        y = df[attr_col].values\n",
    "        # if attr is colors then extract a numerical order from the strings\n",
    "        if isinstance(y[0], str):\n",
    "            get_ints = np.vectorize(lambda x: int(x.split('-')[0]))\n",
    "            y = get_ints(y) \n",
    "            y = y/y.max()\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "        if X_train.ndim==1:\n",
    "            X_train = X_train.reshape(-1,1)\n",
    "            X_test = X_test.reshape(-1,1)\n",
    "\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train, y_train)\n",
    "        y_pred = lr.predict(X_test)\n",
    "        print(f\"\\t R2 : {(lr.score(X_test, y_test))*100:.0f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d1649",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# choose one config file out of the 4, to debug\n",
    "config_fname = config_fnames[3]\n",
    "print(f\"Rerunning the baseline generation the config file: {config_fname}\")\n",
    "\n",
    "N_SAMPLES=5000\n",
    "dataset_path = f\"dataset/toybrains_n{N_SAMPLES}_{os.path.basename(config_fname).replace('.py','')}\"\n",
    "assert os.path.exists(dataset_path+'/train/'), f\"Dataset not found at {dataset_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b9fe91",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if VIZ_DISTS_AFTER: \n",
    "    from utils.vizutils import plot_col_dists, plot_col_counts, show_images\n",
    "    df = pd.read_csv(f'{dataset_path}/train/{os.path.basename(dataset_path)}.csv')\n",
    "    # show the image attributes distributions\n",
    "    cov_cols = ['lbl_y'] +  df.filter(regex='^cov_').columns.tolist()\n",
    "    attr_cols = ['lbl_y'] + [c for c in df.columns if c not in cov_cols and 'ID' not in c] \n",
    "    # print(cov_cols, attr_cols)\n",
    "    plot_col_dists(df, \n",
    "                attr_cols=attr_cols, \n",
    "                cov_cols=cov_cols, \n",
    "                title=f\"Distributions of the image attributes after sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79142e11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "toy = ToyBrainsData(config=config_fname, verbose=verbose)\n",
    "display(toy.show_current_config(subset=['lbl_y',cov_name]))\n",
    "\n",
    "toy.load_generated_dataset(dataset_path)\n",
    "display(toy.draw_dag())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181934c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "## rerun baseline generation\n",
    "if GEN_BASELINES:\n",
    "    import re\n",
    "\n",
    "    N_SAMPLES = 5000\n",
    "    n_samples_test_ood =1000\n",
    "\n",
    "    trials = 5\n",
    "    n_jobs = trials\n",
    "    baseline_models =  BASELINE_MODELS\n",
    "    baseline_metrics = BASELINE_METRICS\n",
    "\n",
    "    baselines_file = f\"{toy.DATASET_DIR}/../baseline_results.csv\"\n",
    "    if os.path.exists(baselines_file):\n",
    "                print(f\"Baseline results file '{baselines_file}' already exists. Overwriting it.\")\n",
    "\n",
    "    df_results_all = []\n",
    "    for model, model_params in baseline_models:\n",
    "        if verbose>0: print(f\"Estimating ground truth associations using {model}({model_params}) model...\")\n",
    "        # if OOD test datasets are available then just estimate the ground truth association using them\n",
    "        test_data_glob = toy.DATASET_DIR.replace('/train/', '/test_*')\n",
    "        re_pattern = r\"_n(\\d+)\"\n",
    "        test_data_glob = re.sub(re_pattern, \"_n*\", test_data_glob)\n",
    "        test_datasets =  {data_dir.rstrip('/').split('/')[-1]: data_dir for data_dir in glob(test_data_glob)}\n",
    "\n",
    "        if verbose>0: print(f\"holdout datasets used for baselining: {list(test_datasets.keys())}\")\n",
    "        \n",
    "        contrib_estimator_args =  dict(\n",
    "                holdout_data=test_datasets,\n",
    "                output_labels_prefix=['lbl'], \n",
    "                model_name=model, model_params=model_params,\n",
    "                outer_CV=trials, n_jobs=n_jobs,\n",
    "                metrics=baseline_metrics,\n",
    "                verbose=verbose)\n",
    "        \n",
    "        # check if there are other test datasets than just 'test_all'\n",
    "        df_results = toy.fit_contrib_estimators(\n",
    "            input_feature_sets=[\"attr_all\"],\n",
    "            **contrib_estimator_args)\n",
    "            \n",
    "        df_results_all.append(df_results)\n",
    "\n",
    "    df_results_all = pd.concat(df_results_all) if len(df_results_all)>1 else df_results_all[0]\n",
    "    df_results_all.to_csv(baselines_file, index=False)\n",
    "    # display(df_results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb8a00",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# df_results_all[['score_holdout_test_all_r2','score_holdout_test_lbl_y_r2', 'score_holdout_test_cov_7_cat2_r2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7587fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "metric='r2'\n",
    "show_scores_decomp(df_results_all, \n",
    "                metric=metric, center_metric=False)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
